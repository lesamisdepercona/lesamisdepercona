<!doctype html><html lang=fr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>PostgreSQL sur Kubernetes : Haute disponibilité et Recupération | Les amis de Percona</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Nous allons approfondir la haute disponibilité, la reprise après desastre et la mise à l'échelle des clusters PostgreSQL avec un minimum d'effort manuel en utilisant Percona Distribution for PostgreSQL Operator"><meta name=generator content="Hugo 0.80.0"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel=stylesheet href=/ananke/css/main.min.css><link rel="shortcut icon" href=/img/favicon.png type=image/x-icon><meta property="og:title" content="PostgreSQL sur Kubernetes : Haute disponibilité et Recupération"><meta property="og:description" content="Nous allons approfondir la haute disponibilité, la reprise après desastre et la mise à l'échelle des clusters PostgreSQL avec un minimum d'effort manuel en utilisant Percona Distribution for PostgreSQL Operator"><meta property="og:type" content="article"><meta property="og:url" content="https://www.lesamisdepercona.fr/posts/postgresql-sur-kubernetes-ha-haute-disponibilite-et-recuperation/"><meta property="og:image" content="https://www.lesamisdepercona.fr/thumbnail2022/article08.jpg"><meta property="article:published_time" content="2022-02-18T00:00:00+00:00"><meta property="article:modified_time" content="2022-02-18T00:00:00+00:00"><meta itemprop=name content="PostgreSQL sur Kubernetes : Haute disponibilité et Recupération"><meta itemprop=description content="Nous allons approfondir la haute disponibilité, la reprise après desastre et la mise à l'échelle des clusters PostgreSQL avec un minimum d'effort manuel en utilisant Percona Distribution for PostgreSQL Operator"><meta itemprop=datePublished content="2022-02-18T00:00:00+00:00"><meta itemprop=dateModified content="2022-02-18T00:00:00+00:00"><meta itemprop=wordCount content="1309"><meta itemprop=image content="https://www.lesamisdepercona.fr/thumbnail2022/article08.jpg"><meta itemprop=keywords content="PostgreSQL,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.lesamisdepercona.fr/thumbnail2022/article08.jpg"><meta name=twitter:title content="PostgreSQL sur Kubernetes : Haute disponibilité et Recupération"><meta name=twitter:description content="Nous allons approfondir la haute disponibilité, la reprise après desastre et la mise à l'échelle des clusters PostgreSQL avec un minimum d'effort manuel en utilisant Percona Distribution for PostgreSQL Operator"><script async src="https://www.googletagmanager.com/gtag/js?id=G-W9SYN1YL24"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','G-W9SYN1YL24');</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-203534013-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-203534013-1');</script></head><body class="ma0 avenir bg-near-white"><header><div class=bg-dark-blue><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib"><img src=/img/logo.png class="w100 mw5-ns" alt="Les amis de Percona"></a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/posts/ title="Articles page">Articles</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about/ title="Qui Sommes Nous? page">Qui Sommes Nous?</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/tags/ title="Tags page">Tags</a></li></ul></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class=mt3><a href="https://www.facebook.com/sharer.php?u=https://www.lesamisdepercona.fr/posts/postgresql-sur-kubernetes-ha-haute-disponibilite-et-recuperation/" class="facebook no-underline" aria-label="share on Facebook"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></a><a href="https://twitter.com/share?url=https://www.lesamisdepercona.fr/posts/postgresql-sur-kubernetes-ha-haute-disponibilite-et-recuperation/&text=PostgreSQL%20sur%20Kubernetes%c2%a0:%20Haute%20disponibilit%c3%a9%20et%20Recup%c3%a9ration" class="twitter no-underline" aria-label="share on Twitter"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.lesamisdepercona.fr/posts/postgresql-sur-kubernetes-ha-haute-disponibilite-et-recuperation/&title=PostgreSQL%20sur%20Kubernetes%c2%a0:%20Haute%20disponibilit%c3%a9%20et%20Recup%c3%a9ration" class="linkedin no-underline" aria-label="share on LinkedIn"><svg height="32" style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30C64 50.568 50.568 64 34 64zM26.354 48.137V27.71h-6.789v20.427H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a></div><h1 class="f1 athelas mt3 mb1">PostgreSQL sur Kubernetes : Haute disponibilité et Recupération</h1><p class=tracked>By <strong>Francis</strong></p><time class="f6 mv4 dib tracked" datetime=2022-02-18T00:00:00Z>February 18, 2022</time>
<span class="f6 mv4 dib tracked">- 7 minutes read</span>
<span class="f6 mv4 dib tracked">- 1309 words</span></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-100-l"><p><img src=/thumbnail2022/article08.jpg alt=thumbnail></p><p><a href=https://www.percona.com/doc/kubernetes-operator-for-postgresql/index.html>Percona Distribution for PostgreSQL Operator</a> vous permet de déployer et de gérer des clusters PostgreSQL hautement disponibles et de qualité production sur Kubernetes avec un minimum d&rsquo;effort manuel. Dans cet article de blog, nous allons approfondir la haute disponibilité, la reprise après desastre et la mise à l&rsquo;échelle des clusters PostgreSQL.</p><h2 id=la-haute-disponibilité>La haute disponibilité</h2><p>Notre manifeste de ressources personnalisé par défaut déploie un cluster PostgreSQL hautement disponible (HA). Les composants clés de la configuration HA sont :</p><ul><li><a href=https://kubernetes.io/docs/concepts/services-networking/service/>Services Kubernetes</a> qui pointent vers pgBouncer et les nœuds de réplique</li><li><a href=https://www.pgbouncer.org/>pgBouncer</a> – un pooler de connexion léger pour PostgreSQL</li><li><a href=https://patroni.readthedocs.io/en/latest/>Patroni</a> - Orchestrator HA pour PostgreSQL</li><li>PostgreSQL - nous avons un nœud principal et 2 nœuds de réplica en secours à chaud par défaut</li></ul><p><img src=/posts/2022/article08/img01.png alt=image01></p><p>Kubernetes est le moyen d&rsquo;exposer votre cluster PostgreSQL aux applications ou aux utilisateurs. Nous avons deux services :</p><ul><li><code>clusterName-pgbouncer</code> – Exposition de votre cluster PostgreSQL via le pooler de connexion pgBouncer. Les lectures et les écritures sont envoyées au nœud principal.</li><li><code>clusterName-replica</code> – Expose directement les nœuds de réplique. Il ne doit être utilisé que pour les lectures. N&rsquo;oubliez pas non plus que les connexions à ce service ne sont pas regroupées. Nous travaillons sur une meilleure solution, où l&rsquo;utilisateur pourrait tirer parti à la fois de la mise en commun des connexions et de la mise à l&rsquo;échelle de la lecture via un seul service.</li></ul><p>Par défaut, nous utilisons le type de service ClusterIP , mais vous pouvez le modifier respectivement dans <code>pgBouncer.expose.serviceType</code> ou <code>pgReplicas.hotStandby.expose.serviceType</code>.</p><p>Chaque conteneur PostgreSQL exécute Patroni qui surveille l&rsquo;état du cluster et, en cas de défaillance du nœud principal, transfère le rôle du nœud principal à l&rsquo;un des nœuds de réplication. PgBouncer sait toujours où se trouve Primary.</p><p>Comme vous le voyez, nous distribuons les composants de cluster PostgreSQL sur différents nœuds Kubernetes . Cela se fait avec des règles d'<a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/>affinité </a>et elles sont appliquées par défaut pour garantir qu&rsquo;une panne de nœud unique n&rsquo;entraîne pas d&rsquo;indisponibilité de la base de données.</p><h2 id=multi-datacenter-avec-multi-az>Multi-Datacenter avec Multi-AZ</h2><p>Bonne conception d&rsquo;architecture consiste à exécuter votre cluster Kubernetes sur plusieurs centres de données. Les clouds publics ont un concept de zones de disponibilité (AZ) qui sont des centres de données dans une région avec une connexion réseau à faible latence entre eux. Habituellement, ces centres de données sont distants d&rsquo;au moins 100 kilomètres les uns des autres afin de minimiser la probabilité d&rsquo;une panne régionale. Vous pouvez tirer parti du déploiement Kubernetes multi-AZ pour exécuter des composants de cluster dans différents centres de données pour une meilleure disponibilité.</p><p><img src=/posts/2022/article08/img02.png alt=image02></p><p>Pour vous assurer que les composants PostgreSQL sont distribués dans les zones de disponibilité, vous devez modifier les règles d&rsquo;affinité. Désormais, cela n&rsquo;est possible qu&rsquo;en modifiant directement les ressources de déploiement :</p><pre><code>$ kubectl edit deploy cluster1-repl2
…
-            topologyKey: kubernetes.io/hostname
+            topologyKey: topology.kubernetes.io/zone
</code></pre><h2 id=mise-à-léchelle>Mise à l&rsquo;échelle</h2><p>La mise à l’échelle de PostgreSQL pour répondre à la demande aux heures de pointe est cruciale pour la haute disponibilité. Notre opérateur vous fournit des outils pour mettre à l&rsquo;échelle les composants PostgreSQL à la fois horizontalement et verticalement.</p><h3 id=mise-à-léchelle-verticale>Mise à l&rsquo;échelle verticale</h3><p>La mise à l&rsquo;échelle verticale consiste à ajouter plus de puissance à un nœud PostgreSQL . La méthode recommandée consiste à modifier les ressources dans la ressource personnalisée (au lieu de les modifier directement dans les objets de déploiement). Par exemple, modifiez les éléments suivants dans le <code>cr.yaml</code> pour obtenir 256 Mo de RAM pour tous les nœuds PostgreSQL Replica :</p><pre><code>  pgReplicas:
    hotStandby:
      resources:
        requests:
-         memory: &quot;128Mi&quot;
+         memory: &quot;256Mi&quot;
</code></pre><p>Appliquer <code>cr.yaml</code> :</p><pre><code>$ kubectl apply -f cr.yaml
</code></pre><p>Utilisez la même approche pour régler d&rsquo;autres composants dans leurs sections correspondantes.</p><p>Vous pouvez également tirer parti <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>de Vertical Pod Autoscaler </a>(VPA) pour réagir automatiquement aux pics de charge. Nous créons une ressource de déploiement pour le nœud principal et chaque nœud de réplication. Les objets VPA doivent cibler ces déploiements. L&rsquo;exemple suivant suivra l&rsquo;une des ressources de déploiement de répliques du cluster1 et la mettra automatiquement à l&rsquo;échelle :</p><pre><code>apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: pxc-vpa
spec:
  targetRef:
    apiVersion: &quot;apps/v1&quot;
    kind:       Deployment
    name:     cluster1-repl1  
    namespace:  pgo
  updatePolicy:
    updateMode: &quot;Auto&quot;
</code></pre><p>Veuillez lire davantage sur VPA et ses fonctionnalités dans la documentation.</p><h3 id=mise-à-léchelle-horizontale>Mise à l&rsquo;échelle horizontale</h3><p>L&rsquo;ajout de nœuds de réplique ou de pgBouncers supplémentaires peut être effectué en modifiant les paramètres de taille dans la ressource personnalisée. Faites le changement suivant dans le <code>cr.yaml</code> par défaut :</p><pre><code>  pgReplicas:
    hotStandby:
-      size: 2
+      size: 3
</code></pre><p>Appliquez la modification pour obtenir un nœud de réplication PostgreSQL supplémentaire :</p><pre><code>$ kubectl apply -f cr.yaml
</code></pre><p>À partir de la version 1.1.0, il est également possible de mettre à l&rsquo;échelle notre cluster à l&rsquo;aide de la commande kubectl scale. Exécutez ce qui suit pour avoir deux nœuds de réplique PostgreSQL dans le cluster 1 :</p><pre><code>$ kubectl scale --replicas=2 perconapgcluster/cluster1
perconapgcluster.pg.percona.com/cluster1 scaled
</code></pre><p>Dans la dernière version, il n&rsquo;est pas encore possible d&rsquo;utiliser <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaler </a>(HPA) et nous le prendrons en charge dans la prochaine. Suivez-nous!</p><h2 id=reprise-après-désastre>Reprise après désastre</h2><p>Il est important de comprendre que la reprise après desastre (DR) n&rsquo;est pas une haute disponibilité. L&rsquo;objectif de DR est d&rsquo;assurer la continuité des activités en cas de catastrophe massive, telle qu&rsquo;une panne de toute la région. La récupération dans de tels cas peut bien sûr être automatisée, mais pas nécessairement - cela dépend strictement des besoins de l&rsquo;entreprise.</p><p><img src=/posts/2022/article08/img03.png alt=image03></p><h2 id=sauvegarde-et-restauration>Sauvegarde et restauration</h2><p>Je pense que c&rsquo;est le protocole de récupération après desastre le plus courant - prenez la sauvegarde, stockez-la dans des locaux tiers, restaurez-la dans un autre centre de données si nécessaire.</p><p>Cette approche est simple, mais s&rsquo;accompagne d&rsquo;un long temps de récupération, surtout si la base de données est volumineuse. Utilisez cette méthode uniquement si elle dépasse vos objectifs de temps de récupération (RTO).</p><p><img src=/posts/2022/article08/img04.png alt=image04></p><p>Notre opérateur gère la sauvegarde et la restauration des clusters PostgreSQL. La reprise après desastre est construite autour de pgBackrest et ressemble à ceci :</p><ol><li>Configurez pgBackrest pour télécharger des sauvegardes sur S3 ou GCS (voir notre <a href=https://www.percona.com/doc/kubernetes-operator-for-postgresql/backups.html>documentation</a> pour plus de détails).</li><li>Créez la sauvegarde manuellement ( <a href=https://www.percona.com/doc/kubernetes-operator-for-postgresql/backups.html#making-on-demand-backup>via pgTask</a> ou assurez-vous qu&rsquo;une sauvegarde planifiée a été créée.</li><li>Une fois le cluster principal défaillant, créez le nouveau cluster dans le centre de données de reprise après desastre. Le cluster doit être exécuté en mode veille et pgBackrest doit pointer vers le même référentiel que le cluster principal :</li></ol><pre><code>spec:
  standby: true
  backup:
  # same config as on original cluster
</code></pre><p>Une fois les données récupérées, l&rsquo;utilisateur peut désactiver le mode veille et basculer l&rsquo;application vers le cluster DR.</p><h2 id=restauration-continue>Restauration continue</h2><p>Cette approche est assez similaire à la précédente : les instances de pgBackrest synchronisent en permanence les données entre deux clusters via le stockage d&rsquo;objets. Cette approche minimise le RTO et vous permet de basculer le trafic de l&rsquo;application vers le site DR presque immédiatement.</p><p><img src=/posts/2022/article08/img05.png alt=image05></p><p>La configuration ici est similaire au cas précédent, mais nous exécutons toujours un deuxième cluster PostgreSQL dans le centre de données Disaster Recovery. En cas de panne du site principal, désactivez simplement le mode veille :</p><pre><code>spec:
  standby: false
</code></pre><p>Vous pouvez utiliser une configuration similaire pour migrer les données vers et depuis Kubernetes . En savoir plus à ce sujet dans l' article de blog <a href=https://www.percona.com/blog/migrating-postgresql-to-kubernetes>Migrating PostgreSQL to Kubernetes </a>.</p><h2 id=conclusion>Conclusion</h2><p>Kubernetes fournissent un service prêt à l&rsquo;emploi et, dans le cas de Percona Distribution for PostgreSQL Operator, l&rsquo;utilisateur obtient un cluster de bases de données hautement disponible de qualité production. De plus, l&rsquo;opérateur fournit des capacités d&rsquo;exploitation au jour 2 et automatise la routine quotidienne.</p><p>Nous vous encourageons à essayer notre opérateur. Consultez notre <a href=https://github.com/percona/percona-postgresql-operator/>référentiel GitHub</a> et consultez la <a href=https://www.percona.com/doc/kubernetes-operator-for-postgresql/index.html>documentation </a>.</p><p>Vous avez trouvé un bogue ou avez une idée de fonctionnalité ? N&rsquo;hésitez pas à le soumettre dans <a href=https://jira.percona.com/browse/K8SPG>JIRA </a>.</p><p>Pour les questions d&rsquo;ordre général, veuillez soulever le sujet dans le <a href=https://forums.percona.com/c/postgresql/percona-kubernetes-operator-for-postgresql/68>forum de la communauté </a>.</p><p>Vous êtes développeur et souhaitez contribuer ? Veuillez lire notre <a href=https://github.com/percona/percona-postgresql-operator/blob/main/CONTRIBUTING.md>CONTRIBUTING.md </a>et envoyer le Pull.</p><p>Source: <a href=https://www.percona.com/blog/postgresql-high-availability-and-disaster-recovery-on-kubernetes>Percona</a></p><ul class=pa0><li class=list><a href=/tags/postgresql class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">PostgreSQL</a></li></ul><div class="mt6 instapaper_ignoref"></div></div></article></main><footer class="bg-dark-blue bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://www.lesamisdepercona.fr/>&copy; Les amis de Percona 2022</a><div></div></div></footer></body></html>